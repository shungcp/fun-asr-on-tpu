#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨
#           This file was automatically generated from src/transformers/models/qwen3/modular_qwen3.py.
#               Do NOT edit this file manually as any edits will be overwritten by the generation of
#             the file from the modular. If any change should be done, please apply the change to the
#                          modular_qwen3.py file directly. One of our CI enforces this.
#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨
# coding=utf-8
# Copyright 2025 The Qwen team, Alibaba Group and the HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from typing import Callable, Optional, Union

import torch
from torch import nn

from transformers.activations import ACT2FN
from transformers.cache_utils import Cache, DynamicCache
from transformers.generation import GenerationMixin
from transformers.integrations import use_kernel_forward_from_hub
from transformers.masking_utils import create_causal_mask, create_sliding_window_causal_mask
from transformers.modeling_flash_attention_utils import FlashAttentionKwargs
# transformers.modeling_utils in older/different versions might not have these generics
# We mock them as they are likely unused for CausalLM ASR inference
try:
    from transformers.modeling_utils import (
        GenericForQuestionAnswering,
        GenericForSequenceClassification,
        GenericForTokenClassification,
    )
except ImportError:
    class GenericForQuestionAnswering: pass
    class GenericForSequenceClassification: pass
    class GenericForTokenClassification: pass
# GradientCheckpointingLayer might be internal or renamed, let's try direct import or fallback
try:
    from transformers.modeling_utils import GradientCheckpointingLayer
except ImportError:
    # Fallback for older transformers: define a dummy mixin or base class
    # Since Qwen3DecoderLayer inherits from it, it must be a class
    # AND it must inherit from nn.Module because Qwen3DecoderLayer doesn't explicitly inherit from nn.Module in the original code
    class GradientCheckpointingLayer(nn.Module):
        def gradient_checkpointing_enable(self, gradient_checkpointing_kwargs=None):
            pass

from transformers.modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast
from transformers.modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update
from transformers.modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel
from transformers.utils import ModelOutput
try:
    from typing import Unpack
except ImportError:
    from typing_extensions import Unpack

# TransformersKwargs is often just a TypedDict or similar, usually not importable from utils
# We define a dummy one if not found to avoid import errors
try:
    from transformers.utils import TransformersKwargs
except ImportError:
    # Fallback: effectively Any or dict
    from typing import Any
    TransformersKwargs = Any
from transformers.utils import auto_docstring, can_return_tuple
from transformers.utils.deprecation import deprecate_kwarg
from transformers.utils.generic import check_model_inputs

# Qwen3Config: Try to import from transformers if available, else Qwen2Config
try:
    from transformers import Qwen2Config as Qwen3Config 
except ImportError:
    from transformers import AutoConfig
    class Qwen3Config(AutoConfig): pass



@use_kernel_forward_from_hub("RMSNorm")
class Qwen3RMSNorm(nn.Module):
    def __init__(self, hidden_size, eps: float = 1e-6) -> None:
        """
        Qwen3RMSNorm is equivalent to T5LayerNorm
        """
        super().__init__()
        self.weight = nn.Parameter(torch.ones(hidden_size))
        self.variance_epsilon = eps

    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
        input_dtype = hidden_states.dtype
        hidden_states = hidden_states.to(torch.float32)
        variance = hidden_states.pow(2).mean(-1, keepdim=True)
        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
        return self.weight * hidden_states.to(input_dtype)

    def extra_repr(self):
        return f"{tuple(self.weight.shape)}, eps={self.variance_epsilon}"


class Qwen3MLP(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.config = config
        self.hidden_size = config.hidden_size
        self.intermediate_size = config.intermediate_size
        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)
        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)
        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)
        self.act_fn = ACT2FN[config.hidden_act]

    def forward(self, x):
        down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
        return down_proj


def rotate_half(x):
    """Rotates half the hidden dims of the input."""
    x1 = x[..., : x.shape[-1] // 2]
    x2 = x[..., x.shape[-1] // 2 :]
    return torch.cat((-x2, x1), dim=-1)


def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):
    """Applies Rotary Position Embedding to the query and key tensors.

    Args:
        q (`torch.Tensor`): The query tensor.
        k (`torch.Tensor`): The key tensor.
        cos (`torch.Tensor`): The cosine part of the rotary embedding.
        sin (`torch.Tensor`): The sine part of the rotary embedding.
        position_ids (`torch.Tensor`, *optional*):
            Deprecated and unused.
        unsqueeze_dim (`int`, *optional*, defaults to 1):
            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and
            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note
            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and
            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes
            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have
            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.
    Returns:
        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.
    """
    # DEBUG: Rotate Type Check
    # print(f"[Debug] RoPE: q={type(q)}/{q.dtype}/{q.device}, cos={type(cos)}/{cos.dtype}/{cos.device}")

    cos = cos.unsqueeze(unsqueeze_dim)
    sin = sin.unsqueeze(unsqueeze_dim)
    # q_embed = (q * cos) + (rotate_half(q) * sin)
    # k_embed = (k * cos) + (rotate_half(k) * sin)
    q_embed = torch.add(torch.mul(q, cos), torch.mul(rotate_half(q), sin))
    k_embed = torch.add(torch.mul(k, cos), torch.mul(rotate_half(k), sin))
    return q_embed, k_embed


def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:
    """
    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,
    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)
    """
    batch, num_key_value_heads, slen, head_dim = hidden_states.shape
    if n_rep == 1:
        return hidden_states
    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)


def eager_attention_forward(
    module: nn.Module,
    query: torch.Tensor,
    key: torch.Tensor,
    value: torch.Tensor,
    attention_mask: Optional[torch.Tensor],
    scaling: float,
    dropout: float = 0.0,
    **kwargs: Unpack[TransformersKwargs],
):
    key_states = repeat_kv(key, module.num_key_value_groups)
    value_states = repeat_kv(value, module.num_key_value_groups)

    # FIX: Cast key/value to query dtype (handle BF16 Cache + F32 Compute)
    key_states = key_states.to(query.dtype)
    value_states = value_states.to(query.dtype)

    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling
    if attention_mask is not None:
        # FIX: Pad attention_mask if it is smaller than attn_weights (e.g. 4096 vs 4130)
        if attention_mask.shape[-1] < attn_weights.shape[-1]:
            pad_len = attn_weights.shape[-1] - attention_mask.shape[-1]
            min_val = torch.finfo(attn_weights.dtype).min
            pads = torch.full((attention_mask.shape[0], attention_mask.shape[1], attention_mask.shape[2], pad_len), min_val, dtype=attention_mask.dtype, device=attention_mask.device)
            attention_mask = torch.cat([attention_mask, pads], dim=-1)
        elif attention_mask.shape[-1] > attn_weights.shape[-1]:
            # FIX: Truncate attention_mask if it is larger than attn_weights (e.g. 291 vs 290)
            # This can happen if GENERATION_MAX_LENGTH mismatch or mask over-padding
            attention_mask = attention_mask[..., :attn_weights.shape[-1]]

        # FIX: Also check Query Length (shape[-2])!
        # Error was: add got incompatible shapes (..., 117, 117), (..., 128, 117)
        if attention_mask.dim() >= 2 and attention_mask.shape[-2] > attn_weights.shape[-2]:
             attention_mask = attention_mask[..., :attn_weights.shape[-2], :]
        
        # FIX: use torch.add to avoid in-place errors or broadcasting issues with +
        attn_weights = torch.add(attn_weights, attention_mask)

    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)
    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)
    attn_output = torch.matmul(attn_weights, value_states)
    attn_output = attn_output.transpose(1, 2).contiguous()

    return attn_output, attn_weights


class Qwen3Attention(nn.Module):
    """Multi-headed attention from 'Attention Is All You Need' paper"""

    def __init__(self, config: Qwen3Config, layer_idx: int):
        super().__init__()
        self.config = config
        self.layer_idx = layer_idx
        self.head_dim = getattr(config, "head_dim", config.hidden_size // config.num_attention_heads)
        self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads
        self.scaling = self.head_dim**-0.5
        self.attention_dropout = config.attention_dropout
        self.is_causal = True

        self.q_proj = nn.Linear(
            config.hidden_size, config.num_attention_heads * self.head_dim, bias=config.attention_bias
        )
        self.k_proj = nn.Linear(
            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias
        )
        self.v_proj = nn.Linear(
            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias
        )
        self.o_proj = nn.Linear(
            config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias
        )
        self.q_norm = Qwen3RMSNorm(self.head_dim, eps=config.rms_norm_eps)  # unlike olmo, only on the head dim!
        self.k_norm = Qwen3RMSNorm(self.head_dim, eps=config.rms_norm_eps)  # thus post q_norm does not need reshape
        self.sliding_window = config.sliding_window if config.layer_types[layer_idx] == "sliding_attention" else None

    @deprecate_kwarg("past_key_value", new_name="past_key_values", version="4.58")
    def forward(
        self,
        hidden_states: torch.Tensor,
        position_embeddings: tuple[torch.Tensor, torch.Tensor],
        attention_mask: Optional[torch.Tensor],
        past_key_values: Optional[Cache] = None,
        cache_position: Optional[torch.LongTensor] = None,
        **kwargs: Unpack[FlashAttentionKwargs],
    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:
        input_shape = hidden_states.shape[:-1]
        # FIX: Avoid -1 in view for JAX compatibility with empty tensors
        # hidden_shape = (*input_shape, -1, self.head_dim)

        
        # Calculate explicit shape
        # query_states: [B, L, H_q * D] -> [B, L, H_q, D]
        num_heads = self.config.num_attention_heads
        num_kv_heads = self.config.num_key_value_heads
        
        query_states = self.q_norm(self.q_proj(hidden_states).view(*input_shape, num_heads, self.head_dim)).transpose(1, 2)
        key_states = self.k_norm(self.k_proj(hidden_states).view(*input_shape, num_kv_heads, self.head_dim)).transpose(1, 2)
        value_states = self.v_proj(hidden_states).view(*input_shape, num_kv_heads, self.head_dim).transpose(1, 2)

        cos, sin = position_embeddings
        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)

        if past_key_values is not None:
            # sin and cos are specific to RoPE models; cache_position needed for the static cache
            cache_kwargs = {"sin": sin, "cos": cos, "cache_position": cache_position}
            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)

        attention_interface: Callable = eager_attention_forward
        if self.config._attn_implementation != "eager":
            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]

        attn_output, attn_weights = attention_interface(
            self,
            query_states,
            key_states,
            value_states,
            attention_mask,
            dropout=0.0 if not self.training else self.attention_dropout,
            scaling=self.scaling,
            sliding_window=self.sliding_window,  # diff with Llama
            **kwargs,
        )

        attn_output = attn_output.reshape(*input_shape, -1).contiguous()
        attn_output = self.o_proj(attn_output)
        return attn_output, attn_weights


class Qwen3DecoderLayer(GradientCheckpointingLayer):
    def __init__(self, config: Qwen3Config, layer_idx: int):
        super().__init__()
        self.hidden_size = config.hidden_size

        self.self_attn = Qwen3Attention(config=config, layer_idx=layer_idx)

        self.mlp = Qwen3MLP(config)
        self.input_layernorm = Qwen3RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        self.post_attention_layernorm = Qwen3RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        self.attention_type = config.layer_types[layer_idx]

    @deprecate_kwarg("past_key_value", new_name="past_key_values", version="4.58")
    def forward(
        self,
        hidden_states: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_values: Optional[Cache] = None,
        use_cache: Optional[bool] = False,
        cache_position: Optional[torch.LongTensor] = None,
        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC
        **kwargs: Unpack[TransformersKwargs],
    ) -> torch.Tensor:
        residual = hidden_states
        hidden_states = self.input_layernorm(hidden_states)
        # Self Attention
        hidden_states, _ = self.self_attn(
            hidden_states=hidden_states,
            attention_mask=attention_mask,
            position_ids=position_ids,
            past_key_values=past_key_values,
            use_cache=use_cache,
            cache_position=cache_position,
            position_embeddings=position_embeddings,
            **kwargs,
        )
        hidden_states = residual + hidden_states

        # Fully Connected
        residual = hidden_states
        hidden_states = self.post_attention_layernorm(hidden_states)
        hidden_states = self.mlp(hidden_states)
        hidden_states = residual + hidden_states
        return hidden_states


@auto_docstring
class Qwen3PreTrainedModel(PreTrainedModel):
    config: Qwen3Config
    base_model_prefix = "model"
    supports_gradient_checkpointing = True
    _no_split_modules = ["Qwen3DecoderLayer"]
    _skip_keys_device_placement = ["past_key_values"]
    _supports_flash_attn = True
    _supports_sdpa = True
    _supports_flex_attn = True

    _can_compile_fullgraph = True
    _supports_attention_backend = True
    _can_record_outputs = {
        "hidden_states": Qwen3DecoderLayer,
        "attentions": Qwen3Attention,
    }


class Qwen3RotaryEmbedding(nn.Module):
    inv_freq: torch.Tensor  # fix linting for `register_buffer`

    def __init__(self, config: Qwen3Config, device=None):
        super().__init__()
        # BC: "rope_type" was originally "type"
        if hasattr(config, "rope_scaling") and isinstance(config.rope_scaling, dict):
            self.rope_type = config.rope_scaling.get("rope_type", config.rope_scaling.get("type"))
        else:
            self.rope_type = "default"
        self.max_seq_len_cached = config.max_position_embeddings
        self.original_max_seq_len = config.max_position_embeddings

        self.config = config
        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]

        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)
        self.register_buffer("inv_freq", inv_freq, persistent=False)
        self.original_inv_freq = self.inv_freq

    @torch.no_grad()
    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)
    def forward(self, x, position_ids):
        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)
        position_ids_expanded = position_ids[:, None, :].float()

        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != "mps" else "cpu"
        with torch.autocast(device_type=device_type, enabled=False):  # Force float32
            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
            emb = torch.cat((freqs, freqs), dim=-1)
            cos = emb.cos() * self.attention_scaling
            sin = emb.sin() * self.attention_scaling

        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)


@auto_docstring
class Qwen3Model(Qwen3PreTrainedModel):
    def __init__(self, config: Qwen3Config):
        super().__init__(config)
        self.padding_idx = config.pad_token_id
        self.vocab_size = config.vocab_size

        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)
        self.layers = nn.ModuleList(
            [Qwen3DecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]
        )
        self.norm = Qwen3RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        self.rotary_emb = Qwen3RotaryEmbedding(config=config)
        self.gradient_checkpointing = False
        self.has_sliding_layers = "sliding_attention" in self.config.layer_types

        # Initialize weights and apply final processing
        self.post_init()

    @check_model_inputs
    @auto_docstring
    def forward(
        self,
        input_ids: Optional[torch.LongTensor] = None,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_values: Optional[Cache] = None,
        inputs_embeds: Optional[torch.FloatTensor] = None,
        use_cache: Optional[bool] = None,
        cache_position: Optional[torch.LongTensor] = None,
        **kwargs: Unpack[TransformersKwargs],
    ) -> BaseModelOutputWithPast:
        if (input_ids is None) ^ (inputs_embeds is not None):
            raise ValueError("You must specify exactly one of input_ids or inputs_embeds")

        # CRITICAL FIX: Enforce Boolean Mask for transformers compliance on TPU
        if attention_mask is not None and torch.is_tensor(attention_mask):
            if attention_mask.dtype != torch.bool:
                # print(f"[Qwen3Model] Casting mask from {attention_mask.dtype} to Bool")
                if attention_mask.is_floating_point():
                    attention_mask = attention_mask > 0.5
                else:
                    attention_mask = attention_mask > 0

        if inputs_embeds is None:
            inputs_embeds = self.embed_tokens(input_ids)

        if use_cache and past_key_values is None:
            past_key_values = DynamicCache(config=self.config)

        if cache_position is None:
            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
            cache_position = torch.arange(
                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
            )

        if position_ids is None:
            position_ids = cache_position.unsqueeze(0)

        # It may already have been prepared by e.g. `generate`
        if not isinstance(causal_mask_mapping := attention_mask, dict):
            # Prepare mask arguments
            # mask_kwargs = {
            #     "config": self.config,
            #     "input_embeds": inputs_embeds,
            #     "attention_mask": attention_mask,
            #     "cache_position": cache_position,
            #     "past_key_values": past_key_values,
            #     "position_ids": position_ids,
            # }
            
            # LOCAL FIX: Implement simple TPU-safe causal mask
            batch_size, seq_len = inputs_embeds.shape[:2]
            dtype, device = inputs_embeds.dtype, inputs_embeds.device
            
            # 1. Create Causal Mask (Bool)
            # Use cache_position if available to determine the triangle
            # But normally we just want a simple causal mask for the current query slice against key history
            # If using StaticCache, we might need full sequence length
            
            # For simplicity in Eager Mode (Run 34):
            # Just make a standard lower-triangular mask
            # But wait, past_key_values adds history.
            
            # Let's trust eager_attention_forward to handle 4D mask if provided?
            # Or just provide the 4D mask here?
            
            # If input is [B, L], mask should be [B, 1, L, L] (or L, TotalL)
            
            min_dtype = torch.finfo(dtype).min
            
            # Use cache_position to determine current indices
            if cache_position is None:
                 past_len = 0
            else:
                 if self.training or (attention_mask is None and not use_cache):
                     causal_mask = None # Ensure defined
                     pass # Use default logic below
                 else:
                     # Static Shape Masking Strategy for TPU/JAX
                     # We unconditionally generate a static mask to avoid dynamic shape errors and graph recompilation.
                     causal_mask = None # Default
                     # Safeguard against empty cache_position (traced empty tensor?)
                     if cache_position.numel() > 0:
                         past_len = cache_position[0]
                     else:
                         past_len = 0
            
            target_len = seq_len + past_len
            
            # Full causal mask for the total sequence
            # But we only need the query part row-wise
            # causal_mask = torch.tril(torch.ones((target_len, target_len), dtype=torch.bool, device=device))
            # causal_mask = causal_mask[past_len : past_len + seq_len, :target_len]
            # [L, TotalL]
            
            # BUT: transformers create_causal_mask does complex things for SDPA vs Eager.
            # We are in Eager Mode.
            
            # Actually, eager_attention_forward often expects the mask to be numeric (0, -inf).
            # But we failed because we passed a Float mask to 'where' in transformers which expects Bool first?
            # Wait, the error was "where expected condition to be boolean".
            # The 'transformers' library creates the mask using 'torch.ones' then calls 'where'.
            
            # Let's create the final Numeric Mask directly!
            # Bypass 'eager_mask' entirely.
            
            causal_4d_mask = None
            if seq_len > 1 or past_key_values is None:
                 # We need masking
                 # Create Bool Causal
                 full_len = past_len + seq_len
                 # Indices: Query [past_len: \dots], Key [0: \dots]
                 # i >= j
                 
                 # Create indices
                 # Robust JAX Masking: Avoid arange(traced) by using cat(ones, tril)
                 # Part 1: History (Old Tokens) - Always visible to current tokens
                 # Shape: [seq_len, past_len]
                 # Robust JAX Masking: Avoid factory/expand with traced shapes.
                 # Use arithmetic broadcasting on safe indices!
                 
                 # 1. Create Safe Indices
                 # q_range: [seq_len, 1]
                 q_range = torch.arange(seq_len, device=device).unsqueeze(1)
                 
                 # 2. History Mask [seq_len, past_len]
                 # We want ALL TRUE.
                 # Broadcasting: [seq_len, 1] + [1, past_len] -> [seq_len, past_len]
                 # logic: (q_range >= 0) & (dummy_past >= 0)
                 # We can't use arange(past_len) because past_len is traced.
                 # But we can use zeros(past_len)? No, zeros also needs shape.
                 # WORKAROUND: Use 'past_key_values' structure if possible?
                 # Or just rely on the fact that JAX/TorchAx supports basic broadcasting better.
                 
                 # ULTIMATE HACK:
                 # If we can't create a tensor of shape (past_len), we are stuck.
                 # But wait, 'position_ids' or 'hidden_states' usually has compatible dimensions?
                 # hidden_states is [B, Seq, Dim].
                 # past_key_values is available.
                 
                 # Let's try the 'ones(1).expand' again but maybe with int args if possible?
                 # The error was 'Abstract tracer value ... in item()'.
                 # This means expand() tried to read the int value of past_len.
                 
                 # BACK TO BASICS:
                 # We don't need 'history_mask' if we just index carefully?
                 # No, we need to concat.
                 
                 # Try using 'torch.full' with proper JAX fallback?
                 # What if we just output the indices and compare?
                 # q_idx = (torch.arange(seq_len) + past_len).unsqueeze(1) # [Seq, 1]
                 # k_idx = torch.arange(full_len).unsqueeze(0)             # [1, Full] <-- FAILS (full_len traced)
                 
                 # How to get [1, Full] indices without arange(traced)?
                 # k_idx_history = torch.arange(past_len) <-- FAILS
                 
                 # IDEA: We only need a mask that is:
                 # [True, True, ..., True (past_len times), True, False, False (triangular)]
                 # Actually, History is ALL True. Current is Triangular.
                 
                 # Can we just use `hidden_states` to get shapes?
                 # We can't trivially get 'past_len' sized tensor without factory.
                 
                 # RE-TRY: use slice of existing tensor?
                 # We don't have a [1, Full] tensor handy.
                 
                 # OKAY, what if we use the Loop Variable approach for Mask?
                 # No, too complex.
                 
                 # Let's pivot: Force CPU Fallback for Shape?
                 # No, tracers are JAX-side.
                 
                 # FINAL ATTEMPT STRATEGY:
                 # Use `jnp.ones` directly? We can't import jnp here easily.
                 # But we can use `torch.zeros(1)` and repeat/tile? 
                 # `repeat` also likely checks values.
                 
                 # Wait, `torchax` MUST support creating tensors of dynamic shape.
                 # The issue might be `expand` implementation in `torchax` specifically.
                 # Try `broadcast_to` if available in torch?
                 
                 # Check `position_ids`!
                 # `position_ids` is [1, Seq] or [B, Seq].
                 # We need Past part.
                 
                 # What if we construct mask for ONLY current part, and assume History is handled by SDPA?
                 # eager_attention_forward adds mask to weights [B, H, Q, K].
                 # If we provide [B, 1, Q, Q] (only current), broadcast will fail against K.
                 
                 # WE MUST MATCH K.
                 
                 # Let's assume `past_key_values` stores the cache.
                 # `past_key_values` has `.get_seq_length()`.
                 
                 # If we use `torch.cat`, we need the history part.
                 # If we simply start with `torch.ones(1, 1)` and `repeat`?
                 # `repeat` takes ints.
                 
                 # Okay, use `torch.zeros(seq_len, full_len)`? Fails.
                 
                 # What if we use `k_idx` logic but build it differently?
                 # k_idx = torch.cat([torch.zeros(past_len), torch.arange(seq_len) + past_len]) ?
                 # `zeros(past_len)` fails.
                 
                 # THERE IS NO WAY to create a tensor of dynamic size `past_len` using standard torch factory in this JIT context?
                 # EXCEPT if we slice a larger tensor.
                 # We don't have one?
                 # We have `past_key_values[0][0]` (Key cache) which is [B, H, MaxLen, D].
                 # We can slice it! `cache[:, :, :past_len, :]`.
                 # But we need [1, Past].
                 # take `cache[0, 0, :past_len, 0]` -> [Past]
                 
                 if past_key_values is not None and len(past_key_values) > 0:
                      ref_tensor = None
                      try:
                          # Try to extract a reference tensor from past_key_values[0]
                          layer_0 = past_key_values[0]
                          if isinstance(layer_0, (list, tuple)):
                              ref_state = layer_0[0] # key
                          elif hasattr(layer_0, "keys"):
                              ref_state = layer_0.keys
                          else:
                              ref_state = layer_0
                          
                          if hasattr(ref_state, "keys") and not isinstance(ref_state, torch.Tensor):
                              ref_tensor = ref_state.keys
                          elif isinstance(ref_state, (list, tuple)):
                              ref_tensor = ref_state[0]
                          else:
                              ref_tensor = ref_state
                      except Exception:
                          ref_tensor = None

                      # Fallback checks
                      if ref_tensor is None:
                          if hasattr(past_key_values, "layers") and len(past_key_values.layers) > 0:
                              try:
                                  ref_state = past_key_values.layers[0]
                                  if hasattr(ref_state, "keys"): ref_tensor = ref_state.keys
                                  elif isinstance(ref_state, (list, tuple)): ref_tensor = ref_state[0]
                                  else: ref_tensor = ref_state
                              except: ref_tensor = None
                          elif hasattr(past_key_values, "key_cache") and len(past_key_values.key_cache) > 0:
                              try:
                                  ref_tensor = past_key_values.key_cache[0]
                              except: ref_tensor = None
                      
                      if ref_tensor is not None:
                          max_len = ref_tensor.shape[2]
                          
                          key_indices = torch.arange(max_len, device=device).unsqueeze(0)
                          query_indices = (torch.arange(seq_len, device=device) + past_len).unsqueeze(-1)
                          mask_cond = key_indices <= query_indices
                          slice_tensor = None
                          
                      elif hasattr(past_key_values, "max_cache_len"):
                           max_len = past_key_values.max_cache_len
                           key_indices = torch.arange(max_len, device=device).unsqueeze(0)
                           query_indices = (torch.arange(seq_len, device=device) + past_len).unsqueeze(-1)
                           mask_cond = key_indices <= query_indices
                           slice_tensor = None
                      else:
                           raise RuntimeError(f"Cannot construct mask. Ref tensor missing. PKV Type: {type(past_key_values)}")
                       
                       # DEBUG PRINT
                       # print(f"[Qwen3Mask] past_len={past_len}, seq_len={seq_len}, max_len={max_len}, device={device}")
                       # if mask_cond is not None:
                       #     print(f"[Qwen3Mask] mask_cond shape={mask_cond.shape}, true_count={mask_cond.long().sum().item() if isinstance(mask_cond, torch.Tensor) else 'N/A'}")


                 # Legacy 'current_mask' and 'torch.cat' logic removed in favor of Static Shape Strategy above.
                 # mask_cond is already computed in the try block.
                 
                 # Ensure mask_cond is a Tensor, not a View (Torchax artifact?)
                 if not isinstance(mask_cond, torch.Tensor):
                      mask_cond = torch.as_tensor(mask_cond, device=device)
                 
                 # Explicitly cast to bool if not already
                 mask_cond = mask_cond.to(dtype=torch.bool)
                 
                 # Combine with attention_mask if present
                 if attention_mask is not None:
                      # JAX/TPU Fix: Verify attention_mask shape matches max_len (pkv length)
                      # If attention_mask is larger (e.g. padded to static limit 128), but pkv is smaller (e.g. 66),
                      # we MUST slice attention_mask to match pkv.
                      # attention_mask is typically [B, 1, 1, Max] or [B, 1, Seq, Max]
                      if attention_mask.shape[-1] > max_len:
                           # print(f"[Debug] Slicing attention_mask {attention_mask.shape} to max_len {max_len}")
                           attention_mask = attention_mask[..., :max_len]
                 
                 # Ensure proper broadcasting [B, 1, Q, K]
                 mask_cond = mask_cond.unsqueeze(0).unsqueeze(0) # [1, 1, Seq, MaxLen]

                 # Final Causal Mask
                 min_val = torch.finfo(inputs_embeds.dtype).min
                 causal_mask = torch.where(mask_cond, 0.0, min_val)
                 
                 # Add attention_mask (Padding) handling? 
                 # If attention_mask is provided, it's usually [B, 1, 1, K] or [B, 1, Q, K].
                 # We should add it? Or combine logically?
                 # Standard logic: causal_mask + attention_mask (where attention_mask is 0/-inf)
                 # Here we built causal_mask as 0/-inf.
                 # If attention_mask is 0/-inf, we can add.
                 # If attention_mask is 1/0 (Keep/Discard), we convert and add/min.
                 
                 if attention_mask is not None:
                     # FIX: Convert Bool Attention Mask to Additive Mask
                     # attention_mask (Bool): True=Keep, False=Mask
                     # Target: 0.0=Keep, min_val=Mask
                     
                     if attention_mask.dtype == torch.bool:
                         addr_mask = torch.zeros_like(attention_mask, dtype=inputs_embeds.dtype)
                         addr_mask = torch.where(attention_mask, 0.0, min_val)
                         attention_mask = addr_mask
                     
                     # Check shapes and broadcast if needed before add
                     # causal_mask is [1, 1, Seq, MaxLen]
                     # attention_mask might be [B, MaxLen] or [B, 1, 1, MaxLen]
                     
                     if attention_mask.dim() == 2:
                          attention_mask = attention_mask.unsqueeze(1).unsqueeze(1)
                     
                     causal_mask = causal_mask + attention_mask
                 
                 causal_mask = causal_mask.to(dtype=inputs_embeds.dtype)
                 # Populate the mapping expected by Qwen3DecoderLayer
                 causal_mask_mapping = { "c": causal_mask, "causal": causal_mask } 
                 
                 # ORIGINAL FAILED LINE:
                 # attn_weights = attn_weights + causal_mask
                 # This implies causal_mask handles both causal and padding.
                 # We return causal_mask here.
                      
                 # causal_mask is already computed effectively above.
                 # We bypass the legacy numeric_mask logic which uses traced shapes (leading to TypeError).
                 
                 # Ensure causal_mask is float (it is)
                 
                 causal_mask_mapping = {
                     "c": causal_mask,
                     "causal": causal_mask,
                     "full_attention": causal_mask, # Ensure Qwen3DecoderLayer finds what it needs
                 }
            # The sliding window alternating layers are not always activated depending on the config
            if self.has_sliding_layers:
                causal_mask_mapping["sliding_attention"] = create_sliding_window_causal_mask(**mask_kwargs)

        hidden_states = inputs_embeds

        # create position embeddings to be shared across the decoder layers
        position_embeddings = self.rotary_emb(hidden_states, position_ids)

        for decoder_layer in self.layers[: self.config.num_hidden_layers]:
            hidden_states = decoder_layer(
                hidden_states,
                attention_mask=causal_mask, # Bypass mapping to avoid JAX string indexing TypeError
                position_ids=position_ids,
                past_key_values=past_key_values,
                use_cache=use_cache,
                cache_position=cache_position,
                position_embeddings=position_embeddings,
                **kwargs,
            )

        hidden_states = self.norm(hidden_states)
        return BaseModelOutputWithPast(
            last_hidden_state=hidden_states,
            past_key_values=past_key_values if use_cache else None,
        )


@auto_docstring
class Qwen3ForCausalLM(Qwen3PreTrainedModel, GenerationMixin):
    _tied_weights_keys = ["lm_head.weight"]
    _tp_plan = {"lm_head": "colwise_rep"}
    _pp_plan = {"lm_head": (["hidden_states"], ["logits"])}

    def __init__(self, config):
        super().__init__(config)
        self.model = Qwen3Model(config)
        self.vocab_size = config.vocab_size
        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)

        # Initialize weights and apply final processing
        self.post_init()

    def prepare_inputs_for_generation(
        self, input_ids, past_key_values=None, attention_mask=None, inputs_embeds=None, **kwargs
    ):
        # CRITICAL FIX for TPU: Force Robust Slicing & Inputs Handling
        



        # 1. Dropping Stale Inputs Embeds (The "1,0" Bug Fix)
        # Only drop if we are truly in DECODING phase (seen_tokens > 0)
        # If seen_tokens == 0, it's prefill, we MUST keep inputs_embeds if provided!
        is_decoding = False
        if past_key_values is not None:
             if hasattr(past_key_values, "get_seq_length"):
                  # Use the authoritative source of truth from Cache
                  if past_key_values.get_seq_length() > 0: 
                      is_decoding = True
             elif isinstance(past_key_values, list) and len(past_key_values) > 0:
                  # DynamicCache / Tuple fallback
                  # If we have past layers, assume decoding? 
                  # But verify layer 0 content?
                  if len(past_key_values[0]) > 0: # Check key cache length
                       # Check if tensor is not empty
                       # But for StaticCache tuple wrapper, it might be pre-allocated zeros?
                       # We rely on previous logic usually.
                       is_decoding = True
        
        # Override: If inputs_embeds is provided and we THINK we are decoding,
        # Double check if input_ids suggests we are actually continuing?
        # If input_ids is 1 token, and inputs_embeds is > 1 token, priority to inputs_embeds?
        # Usually generate() passes input_ids=[last] during decoding.
        
        if is_decoding and inputs_embeds is not None:
             # Only drop if input length suggests we are truly decoding (single token step)
             # If we have a long input, it's likely a prefill or forced run, so we KEEP it.
             if inputs_embeds.shape[1] == 1:
                 inputs_embeds = None 
 

        # 2. Slicing Logic
        if past_key_values is not None and inputs_embeds is None:
            # Only take the last token
            if input_ids.shape[1] > 1:
                input_ids = input_ids[:, -1:]

        position_ids = kwargs.get("position_ids", None)
        
        # 3. Derive Position IDs (Before Padding Mask!)
        if position_ids is None:
            if attention_mask is not None:
                # Standard calculation for FlashAttn/SDPA style
                position_ids = attention_mask.long().cumsum(-1) - 1
                position_ids.masked_fill_(attention_mask == 0, 1)
                
                # FIX for StaticCache / Tuple Cache:
                # If we have past_key_values, and we are decoding (input length 1),
                # we must ensure position_ids reflects the TOTAL length, not just the mask relative length.
                # In Static Cache, attention_mask might be padded to MAX_LEN.
                # cumsum would give [0, 1, ..., MAX_LEN-1] if all 1s.
                # If we slice the last one, we get MAX_LEN-1. This seems correct IF mask is fully valid.
                
                # BUT if mask is NOT fully valid (it has 0s at the end), and we are at step N.
                # We need position_ids = [N].
                
                if past_key_values is not None:
                    # Try to get authoritative length
                    prior_len = 0
                    source = "none"
                    if hasattr(past_key_values, "seen_tokens"):
                         prior_len = past_key_values.seen_tokens
                         source = "seen_tokens"
                    elif hasattr(past_key_values, "get_seq_length"):
                         prior_len = past_key_values.get_seq_length()
                         source = "get_seq_length"
                    elif isinstance(past_key_values, (list, tuple)):
                         # Estimate from shapes? 
                         # Usually [batch, heads, seq, dim]
                         cache_item = past_key_values[0]
                         if hasattr(cache_item, "shape"): # Tensor
                              prior_len = cache_item.shape[2] 
                              source = "tensor_shape"
                         elif isinstance(cache_item, (tuple, list)) and len(cache_item) > 0: # Tuple(K, V)
                              prior_len = cache_item[0].shape[2]
                              source = "tuple_shape"
                    
                    # Debug Print
                    #if inputs_embeds is None and input_ids.shape[1] == 1:
                        #print(f"[Debug] prepare_inputs: prior_len={prior_len} source={source} id(pkv)={id(past_key_values)}")

                    # If we are decoding (input_ids.shape[1] == 1) and prior_len > 0:
                    if inputs_embeds is None and input_ids.shape[1] == 1:
                        # Force usage of prior_len if available
                        # ALWAYS override if we found a length, because StaticCache on TPU depends on this exact sync.
                        if prior_len > 0 or source == "seen_tokens":
                            position_ids = torch.tensor([[prior_len]], dtype=torch.long, device=input_ids.device)
                            #print(f"[Debug] prepare_inputs: Forcing position_ids={prior_len} (source={source})")
            



            else:
                 # No mask provided
                 if past_key_values is not None:
                      # ... similar logic ...
                      pass

            # Only slice position_ids if we are in decoding mode
            if past_key_values and inputs_embeds is None and attention_mask is not None and position_ids.shape[1] > 1:
                # If we fell back to cumsum, slice it.
                position_ids = position_ids[:, -1].unsqueeze(-1)
        
        # 4. Handle Inputs Embeds vs Input IDs
        model_inputs = {}
        if inputs_embeds is not None:
            model_inputs = {"inputs_embeds": inputs_embeds}
            input_len = inputs_embeds.shape[1]
            if position_ids is not None and position_ids.shape[1] > input_len:
                position_ids = position_ids[:, :input_len]
        else:
            model_inputs = {"input_ids": input_ids}

        # 5. Global Mask Enforcement (Static Shape for TPU JIT)
        # Matches GENERATION_MAX_LENGTH used in demo script (dynamically from config)
        GENERATION_MAX_LENGTH = getattr(self.config, "static_sequence_length", 128)
        if attention_mask is not None:
             curr_len = attention_mask.shape[1]
             if curr_len < GENERATION_MAX_LENGTH:
                 pad_len = GENERATION_MAX_LENGTH - curr_len
                 pads = torch.zeros((attention_mask.shape[0], pad_len), dtype=attention_mask.dtype, device=attention_mask.device)
                 attention_mask = torch.cat([attention_mask, pads], dim=1)
             elif curr_len > GENERATION_MAX_LENGTH:
                 attention_mask = attention_mask[:, :GENERATION_MAX_LENGTH]

        model_inputs.update({
            "position_ids": position_ids,
            "past_key_values": past_key_values,
            "use_cache": kwargs.get("use_cache"),
            "attention_mask": attention_mask,
            "cache_position": kwargs.get("cache_position"),
        })
        return model_inputs

    def _update_model_kwargs_for_generation(self, outputs, model_kwargs, is_encoder_decoder=False, **kwargs):
        # Standard update calling super if possible, but safely reimplemented here
        # Update past_key_values
        if "past_key_values" in outputs:
            model_kwargs["past_key_values"] = outputs.past_key_values
        elif "mems" in outputs:
            model_kwargs["past_key_values"] = outputs.mems
        elif "past_buckets_states" in outputs:
            model_kwargs["past_buckets_states"] = outputs.past_buckets_states

        if "token_type_ids" in model_kwargs:
            token_type_ids = model_kwargs["token_type_ids"]
            model_kwargs["token_type_ids"] = torch.cat([token_type_ids, token_type_ids[:, -1].unsqueeze(-1)], dim=-1)

        # CRITICAL FIX: Remove inputs_embeds
        if "inputs_embeds" in model_kwargs:
            # inputs_embeds removed to prevent stale arguments in next step
            del model_kwargs["inputs_embeds"]
            
        return model_kwargs

    @can_return_tuple
    @auto_docstring
    def forward(
        self,
        input_ids: Optional[torch.LongTensor] = None,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_values: Optional[Cache] = None,
        inputs_embeds: Optional[torch.FloatTensor] = None,
        labels: Optional[torch.LongTensor] = None,
        use_cache: Optional[bool] = None,
        cache_position: Optional[torch.LongTensor] = None,
        logits_to_keep: Union[int, torch.Tensor] = 0,
        **kwargs: Unpack[TransformersKwargs],
    ) -> CausalLMOutputWithPast:
        r"""
        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,
            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored
            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.

        Example:

        ```python
        >>> from transformers import AutoTokenizer, Qwen3ForCausalLM

        >>> model = Qwen3ForCausalLM.from_pretrained("Qwen/Qwen3-8B")
        >>> tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen3-8B")

        >>> prompt = "Hey, are you conscious? Can you talk to me?"
        >>> inputs = tokenizer(prompt, return_tensors="pt")

        >>> # Generate
        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)
        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]
        "Hey, are you conscious? Can you talk to me?\nI'm not conscious, but I can talk to you."
        ```"""
        outputs: BaseModelOutputWithPast = self.model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            position_ids=position_ids,
            past_key_values=past_key_values,
            inputs_embeds=inputs_embeds,
            use_cache=use_cache,
            cache_position=cache_position,
            **kwargs,
        )

        if isinstance(outputs, torch.Tensor):
             hidden_states = outputs
        elif isinstance(outputs, (tuple, list)):
             hidden_states = outputs[0]
        else:
             hidden_states = outputs.last_hidden_state
        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss
        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep
        logits = self.lm_head(hidden_states[:, slice_indices, :])

        loss = None
        if labels is not None:
            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)

        return CausalLMOutputWithPast(
            loss=loss,
            logits=logits,
            past_key_values=outputs.past_key_values,
            hidden_states=outputs.hidden_states,
            attentions=outputs.attentions,
        )


class Qwen3ForSequenceClassification(GenericForSequenceClassification, Qwen3PreTrainedModel):
    pass


class Qwen3ForTokenClassification(GenericForTokenClassification, Qwen3PreTrainedModel):
    pass


class Qwen3ForQuestionAnswering(GenericForQuestionAnswering, Qwen3PreTrainedModel):
    base_model_prefix = "transformer"  # For BC, where `transformer` was used instead of `model`


__all__ = [
    "Qwen3ForCausalLM",
    "Qwen3ForQuestionAnswering",
    "Qwen3PreTrainedModel",
    "Qwen3Model",
    "Qwen3ForSequenceClassification",
    "Qwen3ForTokenClassification",
]
